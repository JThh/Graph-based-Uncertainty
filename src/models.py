from abc import ABC, abstractmethod
from openai import OpenAI
import torch
import os
import re

import src.utils as utils

# Consolidated exclusion patterns using regex alternation and common keywords
exclusion_patterns = re.compile(
    r'\b(?:I\s+(?:cannot|was unable to|do not|have no)\s+(?:find|provide|locate|access)|'
    r'(?:No|Insufficient|Cannot|Unable)\s+(?:information|data|details|records|bio|profile|content)|'
    r'(?:I\s+am\s+not\s+sure|Note\s+(?:that|,))|'
    r'(?:No\s+(?:additional|further|relevant)\s+information))\b',
    flags=re.IGNORECASE
)


CLIENT = OpenAI(api_key=os.environ['OPENAI_API_KEY'])


class BaseModel(ABC):
    def __init__(self, model_name, args):
        self.model_name = model_name
        self.args = args

    @abstractmethod
    def generate_given_prompt(self, prompt):
        pass

    @abstractmethod
    def generate_n_given_prompt(self, prompt):
        pass

class OpenAIModel(BaseModel):
    def __init__(self, model_name, args):
        super().__init__(model_name, args)
        if model_name == 'gpt-3.5-turbo':
            self.model_name = 'gpt-3.5-turbo-0125'
        elif model_name == 'gpt-4o':
            self.model_name = 'gpt-4o-mini-2024-07-18'

    def generate_given_prompt(self, prompt):
        # print('Prompt\n', prompt)
        if type(prompt) == str:
            prompt = [{'role': 'user', 'content': prompt}]

        if 'gpt' in self.model_name:
            response = CLIENT.chat.completions.create(
                    model=self.model_name,
                    messages=prompt,
                    max_tokens=200,
                    temperature=0.0,
                )
        else:
            raise NotImplementedError
        
        return {'generation': response.choices[0].message.content, 'prompt': prompt}

    def generate_n_given_prompt(self, prompt):
        if 'gpt' in self.model_name:
            response = CLIENT.chat.completions.create(
                    model=self.model_name,
                    messages=[{'role': 'user', 'content': prompt}],
                    max_tokens=512,
                    temperature=self.args.temperature,
                    n=self.args.num_generations_per_prompt
            )
        else:
            raise NotImplementedError
   
        return {'generation': [choice.message.content for choice in response.choices], 'prompt': prompt}


class Llama3Model(BaseModel):
    def __init__(self, model_name, args):
        super().__init__(model_name, args)
        if 'llama-3-70b-instruct' == model_name:
            self.model, self.tokenizer = utils.load_llama3_70b_model_and_tokenizer()
        elif 'llama-3.1-70b-instruct' == model_name:
            self.model, self.tokenizer = utils.load_llama31_70b_model_and_tokenizer()
        elif 'llama-3.1-8b-instruct' == model_name:
            self.model, self.tokenizer = utils.load_llama31_8b_model_and_tokenizer()
        else:
            raise ValueError("Invalid model name:", model_name)
            
    def post_process_bio(self, generated_text: str, prompt: str) -> str:
        """
        Post-process the generated bio to ensure completeness and remove extraneous content.
        
        Args:
            generated_text (str): The raw text generated by the model.
            prompt (str): The prompt that was sent to the model.
        
        Returns:
            str: The cleaned and validated bio.
        """
        # Remove the prompt from the generated text
        bio = generated_text.replace(prompt, "").strip()
        # Ensure the bio ends with a period. If not, truncate to the last complete sentence.
        if not bio.endswith('.'):
            last_period = bio.rfind('.')
            if last_period != -1:
                bio = bio[:last_period + 1]
        
        # Remove any text after the first bio if multiple bios are generated
        bio = re.split(r'\nTell me a bio of', bio, flags=re.IGNORECASE)[0].strip()
        
        # Remove explanations about inability to find more information using the consolidated pattern
        bio = exclusion_patterns.split(bio)[0].strip()
        
        return bio

    def generate_given_prompt(self, prompt):
        messages = [{"role": "user", "content": prompt}]
        input_ids = self.tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(self.model.device)
        
        torch.cuda.empty_cache()

        terminators = [
            self.tokenizer.eos_token_id,
        ]

        outputs = self.model.generate(
            input_ids, max_new_tokens=512, eos_token_id=terminators, 
            do_sample=True, temperature=0.0001, pad_token_id=self.tokenizer.eos_token_id
        )
        generation = self.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True).strip()
        generation = self.post_process_bio(generation, prompt)

        return {'generation': generation, 'prompt': prompt}

    def generate_n_given_prompt(self, prompt):
        messages = [{"role": "user", "content": prompt}]
        input_ids = self.tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(self.model.device)

        terminators = [
            self.tokenizer.eos_token_id,
        ]

        outputs = self.model.generate(
            input_ids, max_new_tokens=512, eos_token_id=terminators, 
            do_sample=True, temperature=self.args.temperature, 
            num_return_sequences=self.args.num_generations_per_prompt, 
            pad_token_id=self.tokenizer.eos_token_id
        )
        generations = [self.tokenizer.decode(decoded[input_ids.shape[-1]:], skip_special_tokens=True).strip() for decoded in outputs]
        generations = [self.post_process_bio(gen, prompt) for gen in generations]
        return {'generation': generations, 'prompt': prompt}

def get_model(model_name, args):
    if 'gpt' in model_name:
        return OpenAIModel(model_name, args)
    elif 'llama-3' in model_name:
        return Llama3Model(model_name, args)
    else:
        raise NotImplementedError
    
class A:
    pass

if __name__ == '__main__':
    model_name = 'llama-3.1-8b-instruct'
    args = A()
    setattr(args, "num_generations_per_prompt", 6)
    setattr(args, "temperature", 0.7)
    model = get_model(model_name, args)
    prompt = 'What is the meaning of life?'
    results = model.generate_given_prompt(prompt)
    print(results)
    more_results = model.generate_n_given_prompt(prompt)
    print(more_results)
